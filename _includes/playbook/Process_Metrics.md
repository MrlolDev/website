"How many people came to the last event? How many likes did our Facebook post get? What is the average number of visualizations of the newsletter?" These are some of the questions that we ask ourselves on a weekly basis as we try to understand how our teams are doing and how they can improve. We call the answers to these questions our **"metrics" – numbers that help us assess the performance and impact of the work we do**.

Let's take a look at how we use metrics in a couple of our projects.

### Metrics measure a subjective goal

Our newsletter team is responsible for sending out weekly emails that inform and excite students about upcoming events and opportunities. Here are some questions they ask themselves ever so often:

> "In what day of the week should we send the newsletter?"  
> "Do we need to improve the current design of emails?"  
> "Should we change the email subject lines to make them clearer?"  

**In making decisions like these, we must try to frame what we hope to accomplish in terms of relevant metrics.** In the case of the newsletter, we want to increase the number of subscribers, the rate of visualizations per email, the average clicks that a certain link gets etc. **That is because we believe that these indicators are correlated with our underlying goal**, which, as said before, is to inform and excite students about upcoming events and opportunities. We use these metrics before it is impossible to measure directly how much we inform and excite students. So if a new format of subject lines increases the number of clicks, it tells us that engagement has probably gone up, so the newsletter team should probably use it. If a new design does the opposite, engagement has probably decreased; the team might want to roll it back.

The email tool used by the newsletter team (ie. MailChimp) should take care of providing the most important statistics for them. But tracking success is not always that easy. Some teams might need to sweat a bit to collect their data.

### Metrics can be tricky

**Measuring is not always straightforward.** It sometimes requires special effort of teams to track attendance, survey students, write down page stats from Facebook etc. Take, for example, the team responsible for hosting our study sessions. What metrics can they use to measure the success of their events? An obvious place to start would be to count the number of attendees – say, by having a volunteer count the number of students who walk into the room. This metric should certainly correlate with the success of our event, right? Yes, but not always! What happens, for instance, when during a particularly underfunded season our study session team is not able to provide food for the participants? We might find that a lot of the people show up to find no cookies and... tip-toe out of the room less than five minutes later. In this case, the number of attendees in itself no longer paints an accurate picture of how the event went, does it?

Here you might be thinking that what we actually want to measure is the total student-hours spent inside that room. _But who's going to volunteer to measure that?!_ What seems, in theory, like a better metric might be impractical to measure. **If something is too hard to measure, it's of no use to us.**

There is no silver lining here: measuring is very tricky sometimes. **It is up to the individual teams to define what metrics they will use to define success and, if necessary, change to better suit the situations.**

Every now and then, the newsletter team will meet to reevaluate the metrics they use and, in terms those metrics, determine what they want to achieve in what period of time. Let's see how this process works.

# Metrics at YCS

The spreadsheet containing all of our metrics [lives in our shared folder and is available to all of our members](https://docs.google.com/spreadsheets/d/1BLYrQrd-UTvufIzzDQ3Dmyy7skhIQh9CD8W_69eHE_M/edit). Each team has its own tab where they fill in, week after week, their metrics.

Our template tab looks like this:

![](http://i.imgur.com/JZG89ZP.png?1)

**Picking the right metrics won't matter unless we commit to following up on them.** That is why we look at them every week during meetings. During board meetings, team managers will bring the latest numbers and discuss them with the rest of the board.

### Health, Execution and Impact

We like to differentiate between three categories of metrics, according to the purpose they serve.

#### Health metrics

> "What is the average rate of absence in our meetings?"  
> "What rate of our members marked themselves 'dissatisfied' with the work we do?"  
> "How many people attended our retreat?"

_Mens sana in corpore sano_. Only a healthy team can keep doing amazing work in the long run. That's why we have metrics to try to measure how well we are working together over time.

**Health metrics are at the same time the most important ones and the trickiest to measure.**

#### Execution metrics

> "How many speakers did we reach out to?"  
> "How many events did we organize this semester?"  

These questions help us track the work we accomplish to do in objective ways. Execution tasks act as follow up to delegated work.

#### Impact metrics

> "How many responded to our Facebook event?"  
> "What is the view rate of our last newsletter?"  
> "How many students said 'very important' when surveyed about YCS's importance?"

Impact metrics help us assess the impact of our projects. Some projects, especially events, can only measure their impact _after_ they finish.

---

References:

- [https://www.atlassian.com/team-playbook/plays/goals-signals-measures]()
- [https://www.nytimes.com/2016/02/28/magazine/what-google-learned-from-its-quest-to-build-the-perfect-team.html]()
