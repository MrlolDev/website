
"How many people came to the last event? How many likes did our Facebook post get? What is the average number of visualizations of the newsletter?" These are some of the questions that we ask ourselves on a weekly basis as we try to understand how our teams are doing and how they can improve. We call the answers to these questions our "metrics" – numbers that help us assess the performance and impact of the work we do. Let's take a look at how we use metrics in one our projects.

### Newsletter example

Our newsletter team is responsible for sending out weekly emails that inform and excite students about upcoming events and opportunities. Here are some questions they ask themselves ever so often:

> "In what day of the week should we send the newsletter?"  
> "Do we need to improve the current design of emails?"  
> "Should we change the email subject lines to make them clearer?"  

In making decisions like these, we must try to frame what we hope to accomplish in terms of relevant metrics. In the case of the newsletter, we want to increase the number of subscribers, the rate of visualizations per email, the average clicks that a certain link gets etc. That is because we believe that these indicators are correlated with our subjective goals, which are to be useful to students, to provide relevant content for them, to be engaging etc, all of which are otherwise impossible to measure directly. So if a new format of subject line increases the number of clicks, the newsletter team should probably stick with it. If a new design does the opposite, they might want to roll it back. These and many other metrics "define" success for them.

The email tool used by the newsletter team (ie. MailChimp) should take care of providing the most important statistics. But tracking success is not always that easy. Some teams might need to sweat a bit to collect their data.

### Challenges of measuring

Measuring is not always a straightforward task. More often than not, it requires the special effort of teams to track attendance, survey students, write down page stats from Facebook etc. How can one of our members, for instance, measure the success of study session events? They could start by having someone keep track of the number of students who walk in the door. That might measure success for some events, but not always. What if, on a particular day, half of the people who show up stay only for five minutes – perhaps because the Insomnia cookies were already over. Then the number of people who walked in no longer that useful.

The takeaway here is that measuring is usually tricky. It is up to the individual teams to define what metrics they will use to define success and, if necessary, change to better suit the situations.

# Metrics at YCS

We keep all of our metrics in single document inside our shared folder. If you're a member, you can [access it here](https://docs.google.com/spreadsheets/d/1BLYrQrd-UTvufIzzDQ3Dmyy7skhIQh9CD8W_69eHE_M/edit). Each of our teams has its own tab, where they keep track, week after week, of how they are doing. In board meetings, team managers will bring the numbers from the past week, and discuss them with the rest of the board.

Every now and then, the newsletter team will meet to reevaluate the metrics they use and, in terms  those metrics, determine what they want to achieve in what period of time – for us, usually six weeks.


### Impact, execution, and health

It can be useful to differentiate between three categories of metrics, according to the purpose they serve.

#### Impact metrics

> "How many responded to our Facebook event?"  
> "What is the view rate of our last newsletter?"  
> "How many students said 'very important' when surveyed about YCS's importance?"

Impact metrics measure if our projects are having an impact.

#### Execution metrics

> "How many speakers did we reach out to?"  
> "How many events did we organize this semester?"  

These questions help us track the work we accomplish to do in objective ways. Execution tasks act as follow up to delegated work.

#### Health metrics

> "What is the average rate of absence in our meetings?"  
> "What rate of our members marked themselves 'dissatisfied' with the work we do?"
> "How many people attended our retreat?"

The famous Latin phrase _Mens sana in corpore sano_ can be effortlessly translated to "Only a healthy team can do amazing work in the long run". ([relevant](https://www.nytimes.com/2016/02/28/magazine/what-google-learned-from-its-quest-to-build-the-perfect-team.html)). More important than the number of emails we send or the likes we get are the metrics that tell us how well we all work together.

Finally, some metrics might fall into more than one of these three categories. No big deal. We pick whatever we feel makes more sense.

# Following up

**Picking the right metrics won't matter unless we commit to following up on them.** That is why we look at them every week, during our board and team meetings. 

### Metrics Spreadsheet

We keep all of our metrics in single document inside our shared folder. If you're a member, you can [access it here](https://docs.google.com/spreadsheets/d/1BLYrQrd-UTvufIzzDQ3Dmyy7skhIQh9CD8W_69eHE_M/edit). Each team has its own tab, were it keeps track, week after week.

---

References:

- [https://www.atlassian.com/team-playbook/plays/goals-signals-measures]()